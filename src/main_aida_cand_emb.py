# the goal of this module is to generate aida-adapted file with candidate linking embeddings based on the
# embeddings we are using for DWIE
# (that is located in dwie_linker/entity_embeddings/johannes_yamada_31082020/enwiki_200.txt) BUT filtering only
# the links from aida dictionary passed as parameter and doing additional pre-processing, such as adding the
# links that have been changed (Ex: uppercase to lowercase), also the idea is to add links that no longer exist
# but that are redirected to other link (ex: "Ethnic_Germans" that redirects now to "Germans"), for this the idea is
# to use the files from wikipedia dump passed by Johannes (/vib_search/kzaporojets/wikipedia-en/ and also
# uploading to https://cloud.ilabt.imec.be/index.php/apps/files/?dir=/copies/coreflinker/wikipedia-en&fileid=1084433584).
# This module is somewhat related with the experiments done in aida_candidate_links.py.
import argparse
import ast
import itertools
import json
import logging
import os
import pickle

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)
logger = logging.getLogger(__name__)

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--link_embeddings_file', type=str, help='the file path to entity embeddings',
                        default='entity_embeddings/johannes_yamada_31082020/enwiki_200.txt')
    parser.add_argument('--link_embeddings_file_serialized', type=str,
                        help='the file with the serialized wiki links that were extracted from --link_embeddings_file',
                        default='entity_embeddings/johannes_yamada_31082020/enwiki_200_serial.json')
    parser.add_argument('--aida_dictionary_file', type=str,
                        help='the dictionary file of AIDA generated by main_bert_processor_aida.py script when '
                             'generating the AIDA dataset files. ',
                        default='data/aida/'
                                'aida_reannotated/aida-20210402/transformed/spanbert_s384/links_dictionary.json')
    parser.add_argument('--link_embeddings_filtered_file', type=str,
                        help='the output file of embeddings that is a subset of embeddings in --link_embeddings_file '
                             'that are of the entity links only present in --aida_dictionary_file. ',
                        default='entity_embeddings/johannes_yamada_31082020/enwiki_200_aida_filtered.txt')
    parser.add_argument('--wikipedia_page_dump_file', type=str,
                        help='path to the wikipedia page .sql dump',
                        default='data/wikipedia_dumps/johannes-20210413/wikipedia-page-small.sql')
    parser.add_argument('--wikipedia_page_redirects_file', type=str,
                        help='path to the wikipedia page .sql with redirects from one link to another.',
                        default='data/wikipedia_dumps/johannes-20210413/enwiki-20210101-redirect-small.sql')
    parser.add_argument('--wikipedia_page_dump_pickle', type=str,
                        help='path to the wikipedia page .sql with redirects from one link to another.',
                        default='data/wikipedia_dumps/johannes-20210413/wikipedia-page-small.pickle')
    parser.add_argument('--wikipedia_page_redirect_pickle', type=str,
                        help='path to the wikipedia page .sql with redirects from one link to another.',
                        default='data/wikipedia_dumps/johannes-20210413/enwiki-20210101-redirect-small.pickle')

    args = parser.parse_args()

    links_in_embeddings = set()
    links_in_dictionary = dict()

    aida_dictionary_file = args.aida_dictionary_file
    link_embeddings_file_serialized = args.link_embeddings_file_serialized
    link_embeddings_file = args.link_embeddings_file
    link_embeddings_filtered_file = args.link_embeddings_filtered_file
    wikipedia_page_dump_pickle = args.wikipedia_page_dump_pickle
    wikipedia_page_redirect_pickle = args.wikipedia_page_redirect_pickle
    wikipedia_page_dump_file = args.wikipedia_page_dump_file
    wikipedia_page_redirects_file = args.wikipedia_page_redirects_file

    aida_dictionary = json.load(open(aida_dictionary_file, 'rt'))
    nr_added = 0
    if not os.path.exists(link_embeddings_file_serialized):
        for line in open(link_embeddings_file, 'rt'):
            if not line.startswith('1#'):
                nr_added += 1
                if nr_added % 100000 == 0:
                    logging.info('nr added links: %d' % nr_added)
                link_to_add = line[:line.index(' ')].strip()
                assert link_to_add != ''
                links_in_embeddings.add(link_to_add)
        to_serialize = list(sorted(links_in_embeddings))
        json.dump(to_serialize, open(link_embeddings_file_serialized, 'w'))
    else:
        links_in_embeddings = json.load(open(link_embeddings_file_serialized, 'rt'))
        links_in_embeddings = set(links_in_embeddings)

    found = 0
    total = 0
    not_found_list = list()
    do_fixed1 = True
    do_fixed_redirects = True
    not_found_fixed1_list = list()
    logging.info('length of aida_dictionary: %d' % len(aida_dictionary))
    original_to_fixed = dict()
    already_found = set()

    wiki_id_to_wiki_link = dict()
    wiki_link_to_wiki_ids = dict()
    wiki_id_to_redirected_link = dict()
    if do_fixed_redirects:
        # this is on
        wiki_dump_links = set()
        nr_wiki_dumps_parsed = 0
        if not os.path.exists(wikipedia_page_dump_pickle):
            with open(wikipedia_page_dump_file) as infile:
                for line in infile:
                    nr_wiki_dumps_parsed += 1
                    if nr_wiki_dumps_parsed % 1 == 0:
                        logging.info('nr of lines in wikidump parsed: %d' % nr_wiki_dumps_parsed)
                    if line.startswith('INSERT INTO `page` VALUES'):
                        parsed_line = line[line.index('('):].strip()[:-1]
                        parsed_line = parsed_line.replace(',NULL)', ',None)')
                        parsed_line = parsed_line.replace(',NULL,', ',None,')
                        parsed_line = '[' + parsed_line + ']'
                        try:
                            parsed_line = ast.literal_eval(parsed_line)
                        except:
                            with open('output_error.log', 'w') as outerror:
                                outerror.write(parsed_line)
                            raise RuntimeError(' wrong parsing')
                        for curr_register in parsed_line:
                            wiki_id = curr_register[0]
                            wiki_namespace = curr_register[1]
                            wiki_link = curr_register[2]
                            if wiki_id in wiki_id_to_wiki_link:
                                logging.warning(
                                    'WARNING, this wiki id was already processed: %s to %s and now it points to %s' %
                                    (wiki_id, wiki_id_to_wiki_link[wiki_id], wiki_link))
                            wiki_id_to_wiki_link[wiki_id] = wiki_link
                            wiki_dump_links.add(wiki_link)
            pickle.dump(wiki_id_to_wiki_link, open(wikipedia_page_dump_pickle, 'wb'))
        else:
            wiki_id_to_wiki_link = pickle.load(open(wikipedia_page_dump_pickle, 'rb'))

        nr_wiki_dumps_parsed = 0
        if not os.path.exists(wikipedia_page_redirect_pickle):
            with open(wikipedia_page_redirects_file) as infile:
                for line in infile:
                    if line.startswith('INSERT INTO `redirect` VALUES'):
                        nr_wiki_dumps_parsed += 1
                        if nr_wiki_dumps_parsed % 1 == 0:
                            logging.info('nr of lines in wiki redirect parsed: %s' % nr_wiki_dumps_parsed)
                        parsed_line = line[line.index('('):].strip()[:-1]
                        parsed_line = parsed_line.replace('NULL', 'None')
                        parsed_line = '[' + parsed_line + ']'
                        parsed_line = ast.literal_eval(parsed_line)
                        for curr_register in parsed_line:
                            wiki_id = curr_register[0]
                            wiki_namespace = curr_register[1]
                            redirected_link = curr_register[2]
                            wiki_id_to_redirected_link[wiki_id] = redirected_link
                            if wiki_id in wiki_id_to_wiki_link:
                                # one link can be associated with multiple wiki ids (ex: because they are in
                                # different wiki spaces)
                                if wiki_id_to_wiki_link[wiki_id] not in wiki_link_to_wiki_ids:
                                    wiki_link_to_wiki_ids[wiki_id_to_wiki_link[wiki_id]] = list()
                                wiki_link_to_wiki_ids[wiki_id_to_wiki_link[wiki_id]].append(wiki_id)
                            else:
                                logging.debug('following wiki_id from redirects not in the universe: %s' %
                                             wiki_id)
            pickle.dump({'wiki_link_to_wiki_ids': wiki_link_to_wiki_ids,
                         'wiki_id_to_redirected_link': wiki_id_to_redirected_link},
                        open(wikipedia_page_redirect_pickle, 'wb'))
        else:
            pckled = pickle.load(open(wikipedia_page_redirect_pickle, 'rb'))
            wiki_link_to_wiki_ids = pckled['wiki_link_to_wiki_ids']
            wiki_id_to_redirected_link = pckled['wiki_id_to_redirected_link']

    for k, v in aida_dictionary.items():
        assert k in aida_dictionary
        total += 1
        if total % 10000 == 0:
            logging.info('processed of aida dictionary: %s' % total)
        if k in links_in_embeddings:
            if k not in already_found:
                found += 1
            else:
                logging.info('already has been found: %s' % k)
            already_found.add(k)
        else:
            is_fixed = False
            if do_fixed_redirects:
                if k in wiki_link_to_wiki_ids:
                    wiki_ids = wiki_link_to_wiki_ids[k]
                    for wiki_id in wiki_ids:
                        if wiki_id in wiki_id_to_redirected_link:
                            new_link = wiki_id_to_redirected_link[wiki_id]
                            if new_link in links_in_embeddings:
                                if new_link not in original_to_fixed:
                                    original_to_fixed[new_link] = list()
                                original_to_fixed[new_link].append(k)
                                logging.info('fixed by redirect: %s (old) to %s (new)' % (k, new_link))
                                found += 1
                                is_fixed = True
                                break
            if is_fixed:
                continue
            # tries to combine first letter lowercased
            if do_fixed1:
                sp_k = k.split('_')
                if len(sp_k) <= 10:
                    for curr_comb in itertools.product([0, 1], repeat=len(sp_k)):
                        to_search = ''
                        for idx_c, comb in enumerate(curr_comb):
                            curr_token = sp_k[idx_c]
                            if comb == 1 and curr_token[0].isalpha():
                                curr_token = curr_token[0].upper() + curr_token[1:]
                            if comb == 0 and curr_token[0].isalpha():
                                curr_token = curr_token[0].lower() + curr_token[1:]
                            if idx_c == 0:
                                to_search = curr_token
                            else:
                                to_search = to_search + '_' + curr_token
                        if to_search in links_in_embeddings:
                            is_fixed = True
                            not_found_fixed1_list.append({'original': k, 'fixed': to_search})
                            # original_to_fixed might seem the other way around, but it is ok, since the pointer is
                            # from original link in the embeddings file (enwiki_200.txt) to the correct link in
                            # links_dictionary in this case.

                            if k not in already_found and to_search not in aida_dictionary:
                                if to_search not in original_to_fixed:
                                    # list because in theory one to many is possible (ex: multiple redirects to the
                                    # same link)
                                    original_to_fixed[to_search] = list()
                                original_to_fixed[to_search].append(k)
                                found += 1
                                already_found.add(k)
                            elif k in already_found:
                                logging.info('already has been found 2: %s' % k)
                            elif to_search in aida_dictionary:
                                logging.info(
                                    'to_search is already in aida_dictionary: %s for k of: %s' % (to_search, k))
                            break
            if is_fixed:
                continue

            not_found_list.append(k)
    logging.info('total: %s found: %s, %% found: %s' % (total, found, (found / total) * 100))
    logging.info('fixed 1: %s' % len(not_found_fixed1_list))

    # now opens back the embeddings file, reads it line by line and only adds the entries to another (filtered)
    # embeddings file that are present in the links dictionary, mapping the original embedding names to the
    # fixed using some of the tricks above (in original_to_fixed dicionary map)
    nr_processed = 0
    orig_to_fix_fixed = 0
    with open(link_embeddings_filtered_file, 'wt') as out_file:
        for line in open(link_embeddings_file, 'rt'):
            if not line.startswith('1#'):
                nr_processed += 1
                if nr_processed % 100000 == 0:
                    logging.info('NR of processed links: %s' % nr_processed)

                link_to_add = line[:line.index(' ')].strip()
                assert link_to_add != ''
                if link_to_add in aida_dictionary:
                    out_file.write(line)
                if link_to_add in original_to_fixed:
                    out_file.write(line)
                    orig_to_fix_fixed += 1
                    if len(original_to_fixed[link_to_add]) > 1:
                        logging.info('INTERESTING more than two links linked to a single one(%s): %s' %
                                     (link_to_add, original_to_fixed[link_to_add]))
                    for curr_link in original_to_fixed[link_to_add]:
                        new_line = curr_link + line[line.index(' '):]
                        out_file.write(new_line)
    logging.info('original_to_fixed fixed: %s len(original_to_fixed): %s', (orig_to_fix_fixed, len(original_to_fixed)))
    assert orig_to_fix_fixed == len(original_to_fixed)
