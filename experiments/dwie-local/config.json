{
  "dictionaries": {
    "entities": {
      "init": {
        "filename": "data/dwie/spanbert_format/links_dictionary.json",
        "type": "json"
      },
      "append": [
        "###UNKNOWN###"
      ],
      "update": true
    },
    "bert_subtokens": {
      "init": {
        "type": "bert",
        "filename": "bert-base-cased"
      }
    }
  },
  "output_config": {
    "output_content": false,
    "output_tokens": false
  },
  "model": {
    "name": "coreflinker_spanbert_hoi",
    "debug": true,
    "random_embed_dim": 0,
    "rel_after_coref": true,
    "end_to_end_mentions": true,
    "_end_to_end_mentions": "If it is in false, then it is not end-to-end and gold spans will be fed in, and the modules such as coref and linker will get as input gold spans instead of using the spans from the pruner.",
    "spans_single_sentence": false,
    "_spans_single_sentence": "Whether the candidate spans have to come from a single sentence. Potentially can be problematic for mentions wrongly tokenized such as U.S.",
    "use_all_subtoken_spans": false,
    "_use_all_subtoken_spans": "Whether to use all subtoken spans or already pre-filtered bert subtokens so that each one already corresponds to the first or last part of actual words.",
    "pruner": {
      "weight": 1.0,
      "hidden_dim": 300,
      "ffnn_depth": 2,
      "hidden_dropout": 0.3,
      "prune_ratio": 0.2,
      "sort_after_pruning": true,
      "add_pruner_loss": true,
      "no_cross_overlap": true,
      "_no_cross_overlap": "whether should filter out overlapping spans",
      "use_width_prior": true,
      "_use_width_prior": "whether the width embedding has to be additionally used",
      "max_num_extracted_spans": 3000,
      "debug_stats": true
    },
    "text_embedder": {
      "spanbert_embedder_x": {
        "model_path": "embeddings/token_embeddings/spanbert/spanbert_hf_base",
        "fine_tune_bert": true
      }
    },
    "max_span_length": 15,
    "span-extractor": {
      "type": "endpoint",
      "span_embed": 20,
      "dropout": 0.3,
      "average": false,
      "max_span_length": 15,
      "param": "span_extractor.embed.weight",
      "attention_heads": true
    },
    "span-pairs": {
      "span_product": true,
      "num_distance_buckets": 10,
      "dim_distance_embedding": 20,
      "distance_function": "ordering",
      "init_embeddings_std": 0.02
    },
    "spanprop": {
      "type": "attprop",
      "att_prop": 0,
      "hidden_dim": 300,
      "hidden_dropout": 0.3,
      "scorer_type": "opt-ff-pairs",
      "init_weights_std": 0.02,
      "components_ffnn_depth": 0,
      "scorers_ffnn_depth": 2
    },
    "coref": {
      "enabled": false,
      "weight": 1.0,
      "bidirectional": false,
      "filter_singletons_with_pruner": false,
      "filter_singletons_with_ner": false,
      "singletons": true,
      "_singletons": "needed if end_to_end is in false, but we still want to get singletons",
      "corefprop": {
        "type": "ff_pairs",
        "use_distance_prior": true,
        "num_distance_buckets": 10,
        "coref_prop": 0,
        "_hidden_dim": 150,
        "hidden_dim": 300,
        "hidden_dropout": 0.3,
        "update_coref_scores": true
      }
    },
    "coreflinker": {
      "enabled": true,
      "type": "coreflinker",
      "filter_singletons_with_pruner": true,
      "filter_singletons_with_ner": false,
      "filter_singletons_with_matrix": false,
      "filter_only_singletons": true,
      "ignore_no_mention_chains": true,
      "subtract_pruner_for_singletons": true,
      "float_precision": "float64",
      "doc_level_candidates": false,
      "weight": 1.0,
      "model_type": "base-hoi",
      "_model_type": "STILL NOT SURE IF WE WILL NEED THIS IN coreflinker_mtt. The possible values are 'base' and 'super-naive' for now. The 'base' is implemented in OptFFpairsLinkerCorefBase and 'super-naive' is implemented in OptFFpairsLinkerCorefNaive. In theory, 'base' should perform better than 'super-naive'.",
      "nonlinear_function": "arsinh",
      "_nonlinear_function": "The function to apply to the scores. If null, no function is applied. Another function is 'arsinh'.",
      "smart_arsinh": false,
      "pred_arsinh": true,
      "multihead_nil": "multihead_prod",
      "_multihead_nil": "If 'none' just does a single connection from root for each of the NIL spans. If 'first', only connects the first span. If 'multihead_prod' does the multihead using products as explained in https://docs.google.com/presentation/d/1Za2gCNq55gp1MCTlg4p0CN-JGyKjj4WtzxpYJKDZz3E/edit#slide=id.gd02f9cc825_0_419. If 'multihead_old' does the old multihead subtracting matrices, but I think is wrong.",
      "log_inf_mask": true,
      "exp_trick": true,
      "zeros_to_clusters": false,
      "_zeros_to_clusters": "if 0 has to be assigned as a score from root to links and spans",
      "no_nil_in_targets": true,
      "print_debugging": false,
      "enforce_scores": false,
      "min_score_max": -0.1,
      "min_score_min": -100.0,
      "max_score_max": 100.0,
      "max_score_min": 0.1,
      "coreflinker_prop": {
        "type": "default",
        "coref_prop": 0,
        "hidden_dim": 300,
        "hidden_dropout": 0.3,
        "components_ffnn_depth": 0,
        "init_weights_std": 0.02,
        "init_root_std": null,
        "init_zeros_bias": true,
        "scorers_ffnn_depth": 2,
        "use_nonlinearity_components": false,
        "use_nonlinearity_scorers": true,
        "update_coref_scores": true,
        "separate_right_spans_for_ent": false,
        "filter_with_matrix_type": null,
        "_filter_with_matrix_type": "TODO: WHAT IS THE USE OF THIS?? DELETE IF NOT USED!",
        "root_requires_grad": false,
        "_root_requires_grad": "if in true, then the root node embedding in adjacency matrix will be updated as trained. For more details on why root is needed refer to https://www.aclweb.org/anthology/D07-1015.pdf ('Structured Prediction Models via the Matrix-Tree Theorem') paper."
      },
      "entity_embedder": {
        "dict": "entities",
        "type": "yamada-johannes",
        "embed_file": "embeddings/entity_embeddings/yamada/enwiki_200.txt",
        "filtered_file": "embeddings/entity_embeddings/yamada/enwiki_200_filtered_dwie_e2e.txt",
        "load_type": "ent_wordentvec",
        "_load_type": "Indicates the loading type, for example 'wordvec' just loads normally the token embedding file such as glove.6B.100d.txt.gz ; 'word_wordentvec' on the other hand is used to load the word embeddings from the file that contains both word and entity embeddings such as enwiki_200.txt.; 'ent_wordentvec' does the opposite: loads the entities from the file that contains both word and entity embeddings.",
        "what_load": "dictionary",
        "_what_load": "If in 'allvecs', then loads all the vectors from the embeddings file, if 'dictionary' only loads the vectors that are present in the dictionary (for a particular dataset we are training on). In order to evaluate on third party datasets/to put as a service, it has to be configured in 'allvecs' then. When in 'allvecs', also adds the entries into the dictionary.",
        "use_filtered": true,
        "dim": 200,
        "dropout": 0.0,
        "freeze": true,
        "init_unknown": false,
        "init_random": true,
        "backoff_to_lowercase": false
      }
    }
  },
  "optimizer": {
    "optimizer": "adam",
    "iters": 50,
    "batch_size": 1,
    "write-last-model": true,
    "clip-norm": 10.0,
    "model": "last.model",
    "report_frequency": 702,
    "adam_eps": 1e-6,
    "adam_weight_decay": 1e-2,
    "warmup_ratio": 0.1,
    "gradient_accumulation_steps": 1
  },
  "lr-scheduler": {
    "nr_iters_bert_training": -1,
    "_nr_iters_bert_training": "the number of training instances after which bert freezes and stops being trained. For DWIE 1 epoch = 702 instances.",
    "task_start_epoch": 0,
    "_task_start_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end begins.",
    "task_end_epoch": 50,
    "_task_end_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end ends.",
    "bert_start_epoch": 0,
    "_bert_start_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end begins (first bert_warmup_ratio will be applied).",
    "bert_end_epoch": 50,
    "_bert_end_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end ends.",
    "bert_warmup_ratio": 0.1,
    "task_learning_rate_start": 1e-3,
    "task_learning_rate_end": 1e-4,
    "bert_learning_rate_start": 2e-5,
    "bert_learning_rate_end": 0.0
  },
  "dataloader": {
    "type": "dwie_spanbert_hoi",
    "include_nill_in_candidates": false,
    "include_none_in_candidates": false,
    "doc_level_candidates": false,
    "all_spans_candidates": true,
    "candidates_from_dictionary": true,
    "bert_max_segment_len": 384,
    "transformers-x": []
  },
  "datasets": {
    "train": {
      "filename": "data/dwie/spanbert_format/dwie_bert_s384/",
      "tokenize": false,
      "shuffle_candidates": true,
      "load-in-memory": true,
      "tag": "train",
      "train_linker_tag": "all"
    },
    "test": {
      "filename": "data/dwie/spanbert_format/dwie_bert_s384/",
      "tokenize": false,
      "shuffle_candidates": true,
      "load-in-memory": true,
      "tag": "test",
      "train_linker_tag": "all"
    }
  },
  "trainer": {
    "train": "train",
    "version": "spanbert",
    "evaluate": [
      "test"
    ],
    "evaluation_frequency": {
      "train": -1,
      "test": -1
    },
    "loss_frequency": {
      "train": -1,
      "test": -1
    },
    "write-predictions": true
  }
}
