{
  "dictionaries": {
    "entities": {
      "init": {
        "filename": "data/aida+/spanbert_format/links_dictionary.json",
        "type": "json"
      },
      "append": [
        "###UNKNOWN###"
      ],
      "update": true
    },
    "bert_subtokens": {
      "init": {
        "type": "bert",
        "filename": "bert-base-cased"
      }
    }
  },
  "output_config": {
    "output_content": false,
    "output_tokens": false
  },
  "model": {
    "name": "coreflinker_spanbert_hoi",
    "debug": true,
    "random_embed_dim": 0,
    "rel_after_coref": true,
    "end_to_end_mentions": true,
    "_end_to_end_mentions": "If it is in false, then it is not end-to-end and gold spans will be fed in, and the modules such as coref and linker will get as input gold spans instead of using the spans from the pruner.",
    "spans_single_sentence": false,
    "_spans_single_sentence": "Whether the candidate spans have to come from a single sentence. Potentially can be problematic for mentions wrongly tokenized such as U.S.",
    "use_all_subtoken_spans": false,
    "_use_all_subtoken_spans": "Whether to use all subtoken spans or already pre-filtered bert subtokens so that each one already corresponds to the first or last part of actual words.",
    "pruner": {
      "weight": 1.0,
      "hidden_dim": 300,
      "ffnn_depth": 2,
      "hidden_dropout": 0.3,
      "prune_ratio": 0.40,
      "sort_after_pruning": true,
      "add_pruner_loss": true,
      "no_cross_overlap": true,
      "_no_cross_overlap": "whether should filter out overlapping spans",
      "use_width_prior": true,
      "_use_width_prior": "whether the width embedding has to be additionally used",
      "max_num_extracted_spans": 300,
      "debug_stats": true
    },
    "text_embedder": {
      "spanbert_embedder_x": {
        "model_path": "embeddings/token_embeddings/spanbert/spanbert_hf_base",
        "fine_tune_bert": true
      }
    },
    "max_span_length": 15,
    "span-extractor": {
      "type": "endpoint",
      "span_embed": 20,
      "dropout": 0.3,
      "average": false,
      "max_span_length": 15,
      "param": "span_extractor.embed.weight",
      "attention_heads": true
    },
    "span-pairs": {
      "span_product": true,
      "num_distance_buckets": 10,
      "dim_distance_embedding": 20,
      "distance_function": "ordering",
      "init_embeddings_std": 0.02
    },
    "spanprop": {
      "type": "attprop",
      "att_prop": 0,
      "hidden_dim": 300,
      "hidden_dropout": 0.3,
      "scorer_type": "opt-ff-pairs",
      "init_weights_std": 0.02,
      "components_ffnn_depth": 0,
      "scorers_ffnn_depth": 2
    },
    "coref": {
      "enabled": false,
      "weight": 1.0,
      "bidirectional": false,
      "filter_singletons_with_pruner": true,
      "filter_singletons_with_ner": false,
      "singletons": true,
      "_singletons": "needed if end_to_end is in false, but we still want to get singletons",
      "corefprop": {
        "type": "ff_pairs",
        "use_distance_prior": true,
        "num_distance_buckets": 10,
        "coref_prop": 0,
        "hidden_dim": 300,
        "hidden_dropout": 0.3,
        "update_coref_scores": true,
        "init_weights_std": 0.02,
        "components_ffnn_depth": 0,
        "scorers_ffnn_depth": 2
      }
    },
    "coreflinker": {
      "enabled": true,
      "type": "coreflinker_mtt",
      "filter_singletons_with_pruner": true,
      "filter_only_singletons": true,
      "filter_singletons_with_ner": false,
      "filter_singletons_with_matrix": false,
      "ignore_no_mention_chains": true,
      "subtract_pruner_for_singletons": true,
      "float_precision": "float64",
      "doc_level_candidates": false,
      "weight": 1.0,
      "model_type": "base",
      "nonlinear_function": "arsinh",
      "_nonlinear_function": "The function to apply to the scores. If null, no function is applied. Another function is 'arsinh'.",
      "smart_arsinh": false,
      "nil_partition_implementation": true,
      "pred_arsinh": true,
      "multihead_nil": "multihead_prod",
      "_multihead_nil": "If 'none' just does a single connection from root for each of the NIL spans. If 'first', only connects the first span. If 'multihead_prod' does the multihead using products as explained in https://docs.google.com/presentation/d/12vVEcWkg-BygOM_ui1l0jaE_JJ0RDYwW7wFBqPGgkvY/edit#slide=id.g18c18415074_0_78. If 'multihead_old' does the old multihead subtracting matrices, but I think is wrong.",
      "log_inf_mask": true,
      "exp_trick": false,
      "zeros_to_clusters": false,
      "_zeros_to_clusters": "if 0 has to be assigned as a score from root to links and spans",
      "zeros_to_links": true,
      "_zeros_to_links": "if 0 has to be assigned as a score from root to links, BUT NOT to spans",
      "root_link_max_spans_to_link": false,
      "_root_link_max_spans_to_link": "If in true, then the score from root to link will be the maximum score of the spans to that link or 0 (whichever is bigger), the idea is to increase the recall",
      "root_link_min_zero": false,
      "_root_link_min_zero": "Only if root_link_max_spans_to_link in true, makes sure that the minimum score from root to links is 0",
      "_add_pruner_to_root": "If has to add pruner scores to root to span scores, most of the experiments has run with this in true, but not sure if makes sense",
      "add_pruner_to_root": false,
      "no_nil_in_targets": true,
      "print_debugging": false,
      "print_debugging_matrices": false,
      "enforce_scores": false,
      "min_score_max": -0.1,
      "min_score_min": -100.0,
      "max_score_max": 100.0,
      "max_score_min": 0.1,
      "coreflinker_prop": {
        "type": "default",
        "coref_prop": 0,
        "hidden_dim": 300,
        "hidden_dropout": 0.3,
        "components_ffnn_depth": 0,
        "init_root_std": 0.02,
        "init_weights_std": 0.02,
        "init_root_type": "std",
        "init_zeros_bias": true,
        "scorers_ffnn_depth": 2,
        "use_nonlinearity_components": false,
        "use_nonlinearity_scorers": true,
        "update_coref_scores": true,
        "separate_right_spans_for_ent": false,
        "filter_with_matrix_type": null,
        "root_requires_grad": true,
        "_root_requires_grad": "if in true, then the root node embedding in adjacency matrix will be updated as trained. For more details on why root is needed refer to https://www.aclweb.org/anthology/D07-1015.pdf ('Structured Prediction Models via the Matrix-Tree Theorem') paper.",
        "apply_root_dropout": false
      },
      "entity_embedder": {
        "type": "kolitsas",
        "dict": "entities",
        "load_extension": false,
        "embed_file": "embeddings/entity_embeddings/kolitsas/ent_vecs.npy",
        "extension_embed_file": "embeddings/entity_embeddings/kolitsas/extension_entities/ent_vecs.npy",
        "extension_ent_id_to_wiki_id_file": "embeddings/entity_embeddings/kolitsas/extension_entities/nnid2wikiid.txt",
        "ent_id_to_wiki_id_file": "embeddings/entity_embeddings/kolitsas/nnid2wikiid.txt",
        "wiki_id_to_wiki_link_file": "embeddings/entity_embeddings/kolitsas/wiki_name_id_map.txt",
        "entities_universe_file": "embeddings/entity_embeddings/kolitsas/entities_universe.txt",
        "extension_entities_universe_file": "embeddings/entity_embeddings/kolitsas/extension_entities/entities_universe.txt",
        "filtered_file": "",
        "use_filtered": false,
        "load_type": "ent_wordentvec",
        "_load_type": "Indicates the loading type, for example 'wordvec' just loads normally the token embedding file such as glove.6B.100d.txt.gz ; 'word_wordentvec' on the other hand is used to load the word embeddings from the file that contains both word and entity embeddings such as enwiki_200.txt.; 'ent_wordentvec' does the opposite: loads the entities from the file that contains both word and entity embeddings.",
        "what_load": "dictionary",
        "_what_load": "If in 'allvecs', then loads all the vectors from the embeddings file, if 'dictionary' only loads the vectors that are present in the dictionary (for a particular dataset we are training on). In order to evaluate on third party datasets/to put as a service, it has to be configured in 'allvecs' then. When in True, also adds the entries into the dictionary.",
        "dim": 300,
        "refit_ratio": null,
        "norm_clip": 2355.0,
        "dropout": 0.0,
        "freeze": true,
        "init_unknown": false,
        "init_random": true,
        "backoff_to_lowercase": false
      }
    }
  },
  "optimizer": {
    "optimizer": "adam",
    "iters": 50,
    "batch_size": 1,
    "write-last-model": true,
    "clip-norm": 10.0,
    "model": "last.model",
    "report_frequency": 946,
    "adam_eps": 1e-6,
    "adam_weight_decay": 1e-2,
    "warmup_ratio": 0.1,
    "gradient_accumulation_steps": 1
  },
  "lr-scheduler": {
    "nr_iters_bert_training": -1,
    "_nr_iters_bert_training": "the number of training instances after which bert freezes and stops being trained. For DWIE 1 epoch = 702 instances.",
    "task_start_epoch": 0,
    "_task_start_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end begins.",
    "task_end_epoch": 50,
    "_task_end_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end ends.",
    "bert_start_epoch": 0,
    "_bert_start_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end begins (first bert_warmup_ratio will be applied).",
    "bert_end_epoch": 50,
    "_bert_end_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end ends.",
    "bert_warmup_ratio": 0.1,
    "task_learning_rate_start": 1e-3,
    "task_learning_rate_end": 1e-4,
    "bert_learning_rate_start": 2e-5,
    "bert_learning_rate_end": 0.0
  },
  "dataloader": {
    "type": "dwie_spanbert_hoi",
    "include_nill_in_candidates": false,
    "include_none_in_candidates": false,
    "doc_level_candidates": false,
    "all_spans_candidates": true,
    "candidates_from_dictionary": true,
    "bert_max_segment_len": 384,
    "transformers-x": []
  },
  "datasets": {
    "train": {
      "filename": "data/aida+/spanbert_format/train/",
      "tokenize": false,
      "shuffle_candidates": true,
      "load-in-memory": true,
      "tag": "train",
      "train_linker_tag": "all"
    },
    "testa": {
      "filename": "data/aida+/spanbert_format/testa/",
      "tokenize": false,
      "shuffle_candidates": true,
      "load-in-memory": true,
      "tag": "testa",
      "train_linker_tag": "all"
    },
    "testb": {
      "filename": "data/aida+/spanbert_format/testb/",
      "tokenize": false,
      "shuffle_candidates": true,
      "load-in-memory": true,
      "tag": "testb",
      "train_linker_tag": "all"
    }
  },
  "trainer": {
    "train": "train",
    "version": "spanbert",
    "evaluate": [
      "testa",
      "testb"
    ],
    "evaluation_frequency": {
      "train": -1,
      "testa": -1,
      "testb": -1
    },
    "loss_frequency": {
      "train": -1,
      "testa": -1,
      "testb": -1
    },
    "write-predictions": true
  }
}
