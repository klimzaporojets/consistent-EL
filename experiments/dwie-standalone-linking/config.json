{
  "dictionaries": {
    "entities": {
      "init": {
        "filename": "data/dwie/spanbert_format/links_dictionary.json",
        "type": "json"
      },
      "append": [
        "###UNKNOWN###"
      ],
      "update": true
    },
    "bert_subtokens": {
      "init": {
        "type": "bert",
        "filename": "bert-base-cased"
      }
    }
  },
  "output_config": {
    "output_content": false,
    "output_tokens": false
  },
  "model": {
    "name": "coreflinker_spanbert_hoi",
    "debug": true,
    "random_embed_dim": 0,
    "rel_after_coref": true,
    "end_to_end_mentions": true,
    "_end_to_end_mentions": "If it is in false, then it is not end-to-end and gold spans will be fed in, and the modules such as coref and linker will get as input gold spans instead of using the spans from the pruner.",
    "spans_single_sentence": false,
    "_spans_single_sentence": "Whether the candidate spans have to come from a single sentence. Potentially can be problematic for mentions wrongly tokenized such as U.S.",
    "use_all_subtoken_spans": false,
    "_use_all_subtoken_spans": "Whether to use all subtoken spans or already pre-filtered bert subtokens so that each one already corresponds to the first or last part of actual words.",
    "pruner": {
      "weight": 1.0,
      "hidden_dim": 300,
      "ffnn_depth": 2,
      "hidden_dropout": 0.3,
      "prune_ratio": 0.2,
      "sort_after_pruning": true,
      "add_pruner_loss": true,
      "no_cross_overlap": true,
      "_no_cross_overlap": "whether should filter out overlapping spans",
      "use_width_prior": true,
      "_use_width_prior": "whether the width embedding has to be additionally used",
      "max_num_extracted_spans": 3000,
      "debug_stats": true
    },
    "text_embedder": {
      "spanbert_embedder_x": {
        "model_path": "embeddings/token_embeddings/spanbert/spanbert_hf_base",
        "fine_tune_bert": true
      }
    },
    "max_span_length": 15,
    "span-extractor": {
      "type": "endpoint",
      "span_embed": 20,
      "dropout": 0.3,
      "average": false,
      "max_span_length": 15,
      "param": "span_extractor.embed.weight",
      "attention_heads": true
    },
    "span-pairs": {
      "span_product": true,
      "num_distance_buckets": 10,
      "dim_distance_embedding": 20,
      "distance_function": "ordering",
      "init_embeddings_std": 0.02
    },
    "spanprop": {
      "type": "attprop",
      "att_prop": 0,
      "hidden_dim": 300,
      "hidden_dropout": 0.3,
      "scorer_type": "opt-ff-pairs",
      "init_weights_std": 0.02,
      "components_ffnn_depth": 0,
      "scorers_ffnn_depth": 2
    },
    "coref": {
      "enabled": false,
      "weight": 1.0,
      "bidirectional": false,
      "filter_singletons_with_pruner": true,
      "filter_singletons_with_ner": false,
      "singletons": true,
      "_singletons": "needed if end_to_end is in false, but we still want to get singletons",
      "corefprop": {
        "type": "ff_pairs",
        "use_distance_prior": true,
        "num_distance_buckets": 10,
        "coref_prop": 0,
        "hidden_dim": 300,
        "hidden_dropout": 0.3,
        "update_coref_scores": true,
        "init_weights_std": 0.02,
        "components_ffnn_depth": 0,
        "scorers_ffnn_depth": 2
      }
    },
    "linker": {
      "enabled": true,
      "weight": 1.0,
      "hidden_dim": 300,
      "hidden_dropout": 0.3,
      "scorers_ffnn_depth": 2,
      "init_weights_std": 0.02,
      "doc_level_candidates": false,
      "source": "pruned",
      "entity_embedder": {
        "dict": "entities",
        "type": "yamada-johannes",
        "embed_file": "embeddings/entity_embeddings/yamada/enwiki_200.txt.gz",
        "filtered_file": "embeddings/entity_embeddings/yamada/enwiki_200_filtered_dwie_e2e.txt",
        "use_filtered": true,
        "load_type": "ent_wordentvec",
        "_load_type": "Indicates the loading type, for example 'wordvec' just loads normally the token embedding file such as glove.6B.100d.txt.gz ; 'word_wordentvec' on the other hand is used to load the word embeddings from the file that contains both word and entity embeddings such as enwiki_200.txt.; 'ent_wordentvec' does the opposite: loads the entities from the file that contains both word and entity embeddings.",
        "what_load": "dictionary",
        "_what_load": "If in 'allvecs', then loads all the vectors from the embeddings file, if 'dictionary' only loads the vectors that are present in the dictionary (for a particular dataset we are training on). In order to evaluate on third party datasets/to put as a service, it has to be configured in 'allvecs' then. When in True, also adds the entries into the dictionary.",
        "dim": 200,
        "dropout": 0.0,
        "freeze": true,
        "init_unknown": false,
        "init_random": true,
        "backoff_to_lowercase": false
      }
    }
  },
  "optimizer": {
    "optimizer": "adam",
    "iters": 50,
    "batch_size": 1,
    "write-last-model": true,
    "clip-norm": 10.0,
    "model": "last.model",
    "report_frequency": 702,
    "adam_eps": 1e-6,
    "adam_weight_decay": 1e-2,
    "warmup_ratio": 0.1,
    "gradient_accumulation_steps": 1
  },
  "lr-scheduler": {
    "nr_iters_bert_training": -1,
    "_nr_iters_bert_training": "the number of training instances after which bert freezes and stops being trained. For DWIE 1 epoch = 702 instances.",
    "task_start_epoch": 0,
    "_task_start_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end begins.",
    "task_end_epoch": 50,
    "_task_end_epoch": "When (epoch) the lr changing from task_learning_rate_start to task_learning_rate_end ends.",
    "bert_start_epoch": 0,
    "_bert_start_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end begins (first bert_warmup_ratio will be applied).",
    "bert_end_epoch": 50,
    "_bert_end_epoch": "When (epoch) the lr changing from bert_learning_rate_start to bert_learning_rate_end ends.",
    "bert_warmup_ratio": 0.1,
    "task_learning_rate_start": 1e-3,
    "task_learning_rate_end": 1e-4,
    "bert_learning_rate_start": 2e-5,
    "bert_learning_rate_end": 0.0
  },
  "dataloader": {
    "type": "dwie_spanbert_hoi",
    "include_nill_in_candidates": true,
    "include_none_in_candidates": false,
    "doc_level_candidates": false,
    "all_spans_candidates": true,
    "candidates_from_dictionary": true,
    "bert_max_segment_len": 384,
    "transformers-x": []
  },
  "datasets": {
    "train": {
      "filename": "data/dwie/spanbert_format/dwie_bert_s384/",
      "tokenize": false,
      "shuffle_candidates": true,
      "_shuffle_candidates": "originally in true, trying in false to see how the code behaves",
      "load-in-memory": true,
      "tag": "train",
      "train_linker_tag": "all"
    },
    "test": {
      "filename": "data/dwie/spanbert_format/dwie_bert_s384/",
      "tokenize": false,
      "shuffle_candidates": true,
      "_shuffle_candidates": "originally in true, trying in false to see how the code behaves",
      "load-in-memory": true,
      "tag": "test",
      "train_linker_tag": "all"
    }
  },
  "trainer": {
    "train": "train",
    "version": "spanbert",
    "evaluate": [
      "test"
    ],
    "evaluation_frequency": {
      "train": -1,
      "test": -1
    },
    "loss_frequency": {
      "train": -1,
      "test": -1
    },
    "write-predictions": true
  }
}
